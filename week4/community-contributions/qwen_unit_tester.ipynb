{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KLmPflCuh98"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "id": "uJDHWH1uusBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv /content/drive/MyDrive/colab_env"
      ],
      "metadata": {
        "id": "yyj78byJut7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "activating colab_env virtual folder from my google drive\n",
        "for installing necessary libraries to avoid reinstallig them.\n",
        "\"\"\"\n",
        "\n",
        "!source /content/drive/MyDrive/colab_env/bin/activate; pip install bitsandbytes accelerate transformers torch gradio"
      ],
      "metadata": {
        "id": "HSluJWlvx1zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets find the installed packages from our colab env inside the drive\n",
        "# copy your own path into it from your drive/mydrive/colab_env\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/colab_env/lib/python3.11/site-packages\")"
      ],
      "metadata": {
        "id": "E6NvgoDgx4Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "# HF Spaces Optimized Configuration\n",
        "model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
        "\n",
        "# 1. Load tokenizer (unchanged)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 2. Load model with HF Spaces optimizations (key changes)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                  # Critical for CPU deployment\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,      # Added for 4-bit\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval()"
      ],
      "metadata": {
        "id": "EmWS6Yl4x4x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is suitable for quantized version.\n",
        "\n",
        "def generate_unit_test(code: str, framework: str = \"pytest\") -> str:\n",
        "    \"\"\"\n",
        "    Generates unit tests using Qwen's official format, optimized for 4-bit quantization.\n",
        "    Returns ONLY the test code without explanations.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Generate a complete {framework} unit test for this Python code.\n",
        "Follow these rules STRICTLY:\n",
        "1. Include all necessary imports\n",
        "2. Only return the executable test code\n",
        "3. No explanations or markdown formatting\n",
        "4. Cover edge cases\n",
        "\n",
        "Code:\n",
        "```python\n",
        "{code}\n",
        "```\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template with quantization-safe settings\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "    # Quantization-compatible input processing\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
        "\n",
        "    # Generation parameters optimized for 4-bit\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=384,  # Slightly reduced for CPU/quantized performance\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=False,\n",
        "        temperature=0.3,  # Slightly higher than original for quantized model\n",
        "        top_k=40,  # Helps maintain quality with quantization\n",
        "        repetition_penalty=1.15  # Prevents duplicate assertions\n",
        "    )\n",
        "\n",
        "    # Extract and clean output\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    test_code = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Robust output cleaning (preserves imports and assertions)\n",
        "    test_code = test_code.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "    if test_code.startswith(\"import\") or test_code.startswith(\"from\"):\n",
        "        return test_code\n",
        "    return f\"import {framework}\\n\\n\" + test_code  # Fallback import"
      ],
      "metadata": {
        "id": "GLdk-4P0x5Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface\n",
        "\n",
        "with gr.Blocks(title=\"üß™ Function to Unit Tests\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üöÄ Python Function ‚Üí Unit Test Generator\n",
        "    Paste your Python function below and get instant unit tests!\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            function_input = gr.Code(\n",
        "                label=\"Your Python Function\",\n",
        "                language=\"python\",\n",
        "                lines=10,\n",
        "                value=\"def add(a, b):\\n    return a + b\"  # Default example\n",
        "            )\n",
        "            framework = gr.Dropdown(\n",
        "                [\"pytest\", \"unittest\"],\n",
        "                label=\"Test Framework\",\n",
        "                value=\"pytest\"\n",
        "            )\n",
        "            btn = gr.Button(\"Generate Tests\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            test_output = gr.Code(\n",
        "                label=\"Generated Unit Tests\",\n",
        "                language=\"python\",\n",
        "                lines=15,\n",
        "                interactive=True\n",
        "            )\n",
        "\n",
        "    # Examples\n",
        "    examples = [\n",
        "        [\"def multiply(a, b):\\n    return a * b\", \"pytest\"],\n",
        "        [\"def is_even(n):\\n    return n % 2 == 0\", \"unittest\"]\n",
        "    ]\n",
        "    gr.Examples(examples, inputs=[function_input, framework])\n",
        "\n",
        "    btn.click(\n",
        "        fn=generate_unit_test,  # ‚Üê Your original function\n",
        "        inputs=[function_input, framework],\n",
        "        outputs=test_output,\n",
        "        api_name=\"generate\"\n",
        "    )\n",
        "\n",
        "# Launch with sharing\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "AsP3wePSx5a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Congrats!"
      ],
      "metadata": {
        "id": "rlv5mX4Nykgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}